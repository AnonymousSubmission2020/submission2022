{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Dataset, Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import tables\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Linear\n",
    "import copy\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOGraphDataset(Dataset):\n",
    "    def __init__(self, root, save_file, transform=None, pre_transform=None):\n",
    "        r\"\"\"\n",
    "        Initializing the dataset class\n",
    "        \n",
    "        Args:\n",
    "            root (string): the path to 'raw/' folder and where the 'processed/' folder would be created\n",
    "            save_file (string): the file name for storing the graph data object\n",
    "        \"\"\"\n",
    "        self.class_size = 259258\n",
    "        self.total_tags = 48374\n",
    "        self.num_classes = 2\n",
    "        self.save_file = save_file\n",
    "        super(SOGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        r\"\"\"\n",
    "        Listing down the paths where the node embeddings and edges are stored\n",
    "        \"\"\"\n",
    "        paths = ['training/features.h5','val/features.h5','test/features.h5','tags/all_tags.h5','edges.h5'] \n",
    "        return paths \n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        r\"\"\"\n",
    "        Returning the processed file name where the graph data object is stored\n",
    "        \"\"\"\n",
    "        return self.save_file\n",
    "\n",
    "    def download(self):\n",
    "        r\"\"\"\n",
    "        Needed in case graph data has to be downloaded\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tvs_split(self):\n",
    "        r\"\"\"\n",
    "        Splitting the labels into training, validation, and testing sets. Stratified split is used.\n",
    "        \"\"\"\n",
    "        y = np.array([0]*self.class_size + [1]*10*self.class_size)\n",
    "        trainval_y, test_y = train_test_split(y, test_size = 0.2, random_state=42, stratify=y)\n",
    "        train_y, val_y = train_test_split(trainval_y, test_size = 0.25, random_state=42, stratify=trainval_y)\n",
    "        return train_y, val_y, test_y\n",
    "    \n",
    "    def get_masks(self):\n",
    "        r\"\"\"\n",
    "        Making the training, validation, and testing masks for use during model training and evaluation\n",
    "        \"\"\"\n",
    "        train_y, val_y, test_y = self.get_tvs_split()\n",
    "        c1, c2, c3 = len(train_y), len(val_y), len(test_y) \n",
    "        total_nodes = self.class_size * 11 + self.total_tags\n",
    "        train_mask = torch.zeros(total_nodes, dtype=torch.bool)\n",
    "        train_mask[0:c1] = True\n",
    "        val_mask = torch.zeros(total_nodes, dtype=torch.bool)\n",
    "        val_mask[c1:c1+c2] = True\n",
    "        test_mask = torch.zeros(total_nodes, dtype=torch.bool)\n",
    "        test_mask[c1+c2:c1+c2+c3] = True\n",
    "        return train_mask, val_mask, test_mask\n",
    "    \n",
    "    def get_features(self, filename, top_n=None):\n",
    "        r\"\"\"\n",
    "        Reading '.h5' files\n",
    "        \n",
    "        Args:\n",
    "            filename (string): the '.h5' filename to be read\n",
    "            top_n (integer): the number of rows to be loaded from the top, by default all rows are loaded\n",
    "        \"\"\"\n",
    "        f = tables.open_file(filename, mode='r')\n",
    "        if top_n is None:\n",
    "            features = f.root.data[:]\n",
    "        else:\n",
    "            features = f.root.data[:top_n]\n",
    "        f.close()\n",
    "        return features\n",
    "    \n",
    "    def process(self):\n",
    "        r\"\"\"\n",
    "        Making the graph data object and saving it in the 'save_file' path specified while initialization\n",
    "        \"\"\"\n",
    "        f1 = self.get_features(self.raw_paths[0])\n",
    "        f2 = self.get_features(self.raw_paths[1])\n",
    "        f3 = self.get_features(self.raw_paths[2])\n",
    "        f4 = self.get_features(self.raw_paths[3])\n",
    "        node_features = np.vstack((f1, f2, f3, f4))\n",
    "        del f1, f2, f3, f4\n",
    "        print(\"Node features loaded\")\n",
    "        \n",
    "        edges = np.transpose(self.get_features(self.raw_paths[4]))\n",
    "        print(\"Edges loaded\")\n",
    "        \n",
    "        tag_labels = np.array([-1]*self.total_tags)\n",
    "        train_y, val_y, test_y = self.get_tvs_split()\n",
    "        labels = np.hstack((train_y, val_y, test_y, tag_labels))\n",
    "        \n",
    "        train_mask, val_mask, test_mask = self.get_masks() \n",
    "        \n",
    "        data = Data(x = torch.tensor(node_features),\n",
    "                    edge_index = torch.LongTensor(edges),\n",
    "                    y = torch.tensor(labels))\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "        \n",
    "        print(\"Saving graph data object\")\n",
    "        torch.save(data, os.path.join(self.processed_dir, self.save_file))\n",
    "    \n",
    "    def len(self):\n",
    "        r\"\"\"\n",
    "        Returning the number of graphs that are made (here we make a single graph so we return 1)\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        r\"\"\"\n",
    "        Loading the graph data object that was stored in 'save_file' path specified while initialization\n",
    "        \"\"\"\n",
    "        data = torch.load(os.path.join(self.processed_dir, self.save_file))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing the path containing the 'raw/' folder, and where experiment results would be saved\"\"\"\n",
    "\n",
    "path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making the Stackoverflow graph dataset object\"\"\"\n",
    "\n",
    "dataset = SOGraphDataset(root=path, save_file=\"graph_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the save graph data object and printing some graph details\"\"\"\n",
    "\n",
    "data = dataset[0]\n",
    "print(data)\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of features: {dataset.num_node_features}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels1, hidden_channels2, num_classes):\n",
    "        r\"\"\"\n",
    "        Initializing the graph neural network\n",
    "        \n",
    "        Args:\n",
    "            hidden_channels1 (integer): the number of output channels after the first convolution layer\n",
    "            hidden_channels2 (integer): the number of output channels after the second convolution layer\n",
    "            num_classes (integer): the output dimension of the classification layer\n",
    "        \"\"\"\n",
    "        super(SOGCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels1, improved=True)\n",
    "        self.conv2 = GCNConv(hidden_channels1, hidden_channels2, improved=True)\n",
    "        self.out = Linear(hidden_channels2, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        r\"\"\" \n",
    "        Forward funtion to run on the input graph node features and edges\n",
    "        \n",
    "        Args:\n",
    "            x (numpy.ndarray): the node features of the graph data object\n",
    "            edge_index (numpy.ndarray): the edge index of the graph data object\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = F.softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing model hyperparameters\"\"\"\n",
    "\n",
    "hc1 = 64\n",
    "hc2 = 64\n",
    "learning_rate = 1e-2\n",
    "decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing the graph neural network object and displaying the number of model parameters\"\"\"\n",
    "\n",
    "model = SOGCN(hidden_channels1 = hc1, hidden_channels2 = hc2, num_classes = dataset.num_classes)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# model = model.float().cuda()\n",
    "model = model.float()\n",
    "print(model)\n",
    "print(f\"Number of model parameters: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing optimizer and loss function\"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate,\n",
    "                             weight_decay=decay)\n",
    "\n",
    "train_y, _, _ = dataset.get_tvs_split()\n",
    "weights = len(train_y)/(2*np.bincount(train_y))\n",
    "weights = torch.tensor(weights).float()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing the input to the model\"\"\"\n",
    "\n",
    "data_x = data.x.float().cuda()\n",
    "data_edge = data.edge_index.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initializing some more variables needed while training\"\"\"\n",
    "\n",
    "train_losses = []\n",
    "val_losses = [np.inf]\n",
    "models_path = os.path.join(path, \"models\")\n",
    "os.mkdir(models_path)\n",
    "logs = open(f\"{path}logs.txt\",\"w\")\n",
    "epochs_done = 0\n",
    "epochs = 500\n",
    "eval_step = 10\n",
    "model_step = 100\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training and evaluating the model\"\"\"\n",
    "\n",
    "for epoch in tqdm(range(epochs_done, epochs_done+epochs)):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x=data_x, edge_index=data_edge).cpu().float()\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    logs.write(f\"Epoch: {epoch}, Train Loss: {loss}\\n\")\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if epoch % eval_step == eval_step-1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = out.argmax(dim=1)\n",
    "            val_loss = criterion(out[data.val_mask].detach(), data.y[data.val_mask])\n",
    "            logs.write(\"Validation\\n\")\n",
    "            logs.write(f\"Val Loss: {val_loss}\\n\")\n",
    "            logs.write(f\"{classification_report(data.y[data.val_mask], pred[data.val_mask], digits=7)}\\n\\n\")\n",
    "        \n",
    "        if float(val_loss)<=min(val_losses):\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), f\"{models_path}/model_{epoch+1}.pt\")\n",
    "        val_losses.append(float(val_loss))\n",
    "    \n",
    "    if epoch % model_step == model_step-1:\n",
    "        torch.save(model.state_dict(), f\"{models_path}/model_{epoch+1}.pt\")\n",
    "\n",
    "    del out\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing the trained model\"\"\"\n",
    "\n",
    "logs.write(\"\\nTesting\\n\")\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    out = model(data_x, data_edge).cpu().float()\n",
    "    test_pred = out.argmax(dim=1)[data.test_mask]\n",
    "    test_loss = criterion(out[data.test_mask].detach(), data.y[data.test_mask])\n",
    "    logs.write(\"Test Loss: {test_loss}\\n\")\n",
    "    logs.write(f\"{classification_report(data.y[data.test_mask], pred[data.test_mask], digits=7)}\\n\")\n",
    "    _, _, test_y = dataset.get_tvs_split()\n",
    "    logs.write(f\"cohen kappa: {cohen_kappa_score(test_y, test_pred)}\\n\")\n",
    "logs.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.8",
   "language": "python",
   "name": "env3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
